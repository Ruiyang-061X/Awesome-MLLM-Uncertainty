# Awesome MLLM Uncertainty [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

### :star::star::star: If you find this repo useful, please star it!

## Discussion

üèÑüèÑüèÑ Welcome to join our MLLM uncertainty discussion group (the left QR code)! Or add my WeChat (the right QR code) to enter the group if the group QR code expires~

<div style="display: flex; justify-content: center; align-items: center;">
    <img src=".asset/WeChat_Group.png" width="25%" style="margin-right: 10px;">
    <img src=".asset/WeChat.png" width="25%" style="margin-left: 10px;">
</div>

## Awesome List

+ **DropoutDecoding** [From Uncertainty to Trust: Enhancing Reliability in Vision-Language Models with Uncertainty-Guided Dropout Decoding](https://arxiv.org/abs/2412.06474) (9 Dec 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.06474)
  [![Star](https://img.shields.io/github/stars/kigb/DropoutDecoding.svg?style=social&label=Star)](https://github.com/kigb/DropoutDecoding)

+ **VL-Uncertainty** [VL-Uncertainty: Detecting Hallucination in Large Vision-Language Model via Uncertainty Estimation](https://arxiv.org/abs/2411.11919) (18 Nov 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.11919)
  [![Star](https://img.shields.io/github/stars/Ruiyang-061X/VL-Uncertainty.svg?style=social&label=Star)](https://github.com/Ruiyang-061X/VL-Uncertainty)

+ **MUB** [Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios](https://arxiv.org/abs/2411.02708) (5 Nov 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.02708)
  [![Star](https://img.shields.io/github/stars/Yunkai696/MUB.svg?style=social&label=Star)](https://github.com/Yunkai696/MUB)
  
+ **CrossPred-LVLM** [Can We Predict Performance of Large Models across Vision-Language Tasks?](https://arxiv.org/abs/2410.10112) (14 Oct 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.10112)
  [![Star](https://img.shields.io/github/stars/qinyu-allen-zhao/crosspred-lvlm.svg?style=social&label=Star)](https://github.com/qinyu-allen-zhao/crosspred-lvlm)
  
+ [Reference-free Hallucination Detection for Large Vision-Language Models](https://arxiv.org/abs/2408.05767) (11 Aug 2024, EMNLP 2024 Findings)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2408.05767)
  [![Star](https://img.shields.io/github/stars/Ruiyang-061X/VL-Uncertainty.svg?style=social&label=Star)](https://github.com/Ruiyang-061X/VL-Uncertainty)

+ **Semantic Entropy** [Detecting Hallucinations in Large Language Models Using Semantic Entropy](https://www.nature.com/articles/s41586-024-07421-0) (19 Jun 2024, Nature)
  [![Star](https://img.shields.io/github/stars/jlko/semantic_uncertainty.svg?style=social&label=Star)](https://github.com/jlko/semantic_uncertainty)

+ **UAL** [Uncertainty Aware Learning for Language Model Alignment](https://arxiv.org/abs/2406.04854) (7 Jun 2024, ACL 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.04854)

+ **HIO** [Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization](https://arxiv.org/abs/2405.15356) (24 May 2024, NeurIPS 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.15356)
  [![Star](https://img.shields.io/github/stars/BT-C/HIO.svg?style=social&label=Star)](https://github.com/BT-C/HIO)

+ [Overconfidence is Key: Verbalized Uncertainty Evaluation in Large Language and Vision-Language Models](https://arxiv.org/abs/2405.02917) (5 May 2024, TrustNLP 2024))
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.02917)

+ **Consistency and Uncertainty** [Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering](https://arxiv.org/abs/2404.10193) (16 Apr 2024, CVPR 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.10193)

+ **UPD** [Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models](https://arxiv.org/abs/2403.20331) (29 Mar 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.20331)
  [![Star](https://img.shields.io/github/stars/AtsuMiyai/UPD.svg?style=social&label=Star)](https://github.com/AtsuMiyai/UPD)

+ **ICD** [Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding](https://arxiv.org/abs/2403.18715) (27 Mar 2024, ACL 2024 Findings)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.18715)

+ **The First to Know** [The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?](https://arxiv.org/abs/2403.09037) (14 Mar 2024, ECCV 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.09037)
  [![Star](https://img.shields.io/github/stars/Qinyu-Allen-Zhao/LVLM-LP.svg?style=social&label=Star)](https://github.com/Qinyu-Allen-Zhao/LVLM-LP)

+ **VLM-Uncertainty-Bench** [Uncertainty-Aware Evaluation for Vision-Language Models](https://arxiv.org/abs/2402.14418) (22 Feb 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.14418)
  [![Star](https://img.shields.io/github/stars/EnSec-AI/VLM-Uncertainty-Bench.svg?style=social&label=Star)](https://github.com/EnSec-AI/VLM-Uncertainty-Bench)

+ **LogicCheckGPT** [Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2402.11622) (18 Feb 2024, ACL 2024 Findings)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.11622)
  [![Star](https://img.shields.io/github/stars/CRIPAC-DIG/LogicCheckGPT.svg?style=social&label=Star)](https://github.com/CRIPAC-DIG/LogicCheckGPT)

+ **UQ_ICL** [Uncertainty Quantification for In-Context Learning of Large Language Models](https://arxiv.org/abs/2402.10189) (15 Feb 2024, NAACL 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.10189)
  [![Star](https://img.shields.io/github/stars/lingchen0331/UQ_ICL.svg?style=social&label=Star)](https://github.com/lingchen0331/UQ_ICL)

+ **LLM-Uncertainty-Bench** [Benchmarking LLMs via Uncertainty Quantification](https://arxiv.org/abs/2401.12794) (23 Jan 2024, NeurIPS 2024 Datasets & Benchmarks)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2401.12794)
  [![Star](https://img.shields.io/github/stars/smartyfh/LLM-Uncertainty-Bench.svg?style=social&label=Star)](https://github.com/smartyfh/LLM-Uncertainty-Bench)

+ **VCD** [Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding](https://arxiv.org/abs/2311.16922) (28 Nov 2023, CVPR 2024 Highlight)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.16922)
  [![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/VCD.svg?style=social&label=Star)](https://github.com/DAMO-NLP-SG/VCD)

+ **MAP** [MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model](https://arxiv.org/abs/2210.05335) (11 Oct 2022, CVPR 2023)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2210.05335)
  [![Star](https://img.shields.io/github/stars/IIGROUP/MAP.svg?style=social&label=Star)](https://github.com/IIGROUP/MAP)

+ [Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs](https://arxiv.org/abs/2306.13063) (22 Jun 2023, ICLR 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.13063)
  [![Star](https://img.shields.io/github/stars/MiaoXiong2320/llm-uncertainty.svg?style=social&label=Star)](https://github.com/MiaoXiong2320/llm-uncertainty)

+ **Semantic Uncertainty** [Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation](https://arxiv.org/abs/2302.09664) (19 Feb 2023, ICLR 2023 Spotlight)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2302.09664)
  [![Star](https://img.shields.io/github/stars/lorenzkuhn/semantic_uncertainty.svg?style=social&label=Star)](https://github.com/lorenzkuhn/semantic_uncertainty)
